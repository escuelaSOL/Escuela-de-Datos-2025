---
title: "Clase 6: Herramientas de extracción de datos web"
author: "Fundación SOL"
date: 06/08/2025
date-format: "dddd, D MMM , YYYY"
lang: es
format:
  revealjs:
    theme: night
    incremental: true
    logo: img/logoblanco.png
project:
  type: website
  output-dir: docs
---

## Extracción de datos web

¿Qué es y para qué sirve?

-   La extracción de datos web permite obtener grandes volúmenes de datos, o información estratégica, contenida en sitios web. 
-   Esto permite la actualización automatizada de datos a partir de una fuente contenida en un sitio web.
-   Generalmente se realiza con lenguajes de programación como Python, no obstante, R cuenta también con herramientas para estos fines. 

## Extracción de datos web hoy
-   Cada vez se ha hecho más desafiante el ejercicio de obtener información de sitios web mediante web scraping. 
-   Actualmente existen barreras relevantes par la extracción automatizada de datos, como el rastreo de la IP que hace el requerimiento y los identificadores de actividad humana, entre otras medidas de seguridad. 
-   Esto obliga al uso de proxys y otros mecanismos. Cada vez es menos *automatizable*.

## Extracción de datos web

Aspectos básicos: HTML

-   Las páginas web generalmente están escritas en **HTML**, un lenguaje desarrollado en 1989 que permite a los navegadores web "interpretar" y "proyectar" una imagen del sitio.
-   **HTML** es un lenguaje basado en etiquetas, con una apertura `<element>` y un cierre `</element>`. 
-   Los `<div>` `</div>` permiten agrupar elementos distintos en un mismo contenedor. 

---

::: {#fig-sitioweb}
[![](img/sitioweb.png){width="100%"}](http://info.cern.ch/hypertext/WWW/TheProject.html)

[World Wide Web 1991 - TheProject.html](http://info.cern.ch/hypertext/WWW/TheProject.html).
:::

---

::: {#fig-TIM}
![](img/tim_berners_lee.jpeg){width="90%"}

Tim Berners-Lee.
:::

---

::: {#fig-Fwebserver}
[![](img/First_Web_Server.jpg){width="90%"}]

Primer servidor de internet en CERN.
:::


## Veamos un ejemplo simple con nuestro sitio web 

-   Ingresaremos al sitio de la escuela y extraeremos una tabla para comprender de qué se trata en base a un ejemplo simple. 
-   Generalmente los procesos de extracción de datos requerirán varias pruebas y procesos posteriores de *limpieza* de los datos que hemos logrado extraer. 
-   En la mayoría de los casos el formato de origen no ha sido pensado para análisis de datos.

---

::: {#fig-escueladatos}
[![](img/esc-datos.png){width="72%"}](https://aulavirtual.escuelasol.cl/escuela-de-datos#)

[(Escuela SOL)](https://escuelasol.cl/).
:::

## Activar herramientas desarrollador 

-   Un primer paso es activar en nuestro navegador el *modo desarrollador* que nos permitirá observar la estructura del sitio web. Generalmente esto se puede seleccionar en el menú del navegador. 
-   De todas formas también es posible explorar los elementos del sitio web desde la consola. 

## Herramientas desarrollo

::: {#fig-desarrollador}
[![](img/desarrolla.png){width="80%"}](https://aulavirtual.escuelasol.cl/escuela-de-datos#scrollTop=0)

Escuela de Datos con "Herramientas para Desarrolladores"
:::

---

## Podemos también observar desde la consola

Usaremos la librería *rvest* 

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
library('rvest')
escuela<-read_html("https://aulavirtual.escuelasol.cl/escuela-de-datos") 
escuela
```


## Librería rvest 

-   [rvest](https://rvest.tidyverse.org/) helps you scrape (or harvest) data from web pages. It is designed to work with [magrittr](https://github.com/tidyverse/magrittr) to make it easy to express common web scraping tasks, inspired by libraries like [beautiful soup](https://www.crummy.com/software/BeautifulSoup/) and [RoboBrowser](http://robobrowser.readthedocs.io/en/latest/readme.html).

-   If you're scraping multiple pages, I highly recommend using rvest in concert with polite. The polite package ensures that you’re respecting the [robots.txt](https://en.wikipedia.org/wiki/Robots_exclusion_standard) and not hammering the site with too many requests.

---

::: {#fig-tablaescuela}
[![](img/tablaescuela.png){width="66%"}](https://aulavirtual.escuelasol.cl/escuela-de-datos#scrollTop=0)

Tabla Programa Escuela de datos
:::

## Exploramos el código html

Buscamos las tablas presentes en el "documento" con *html_element*.

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
escuela %>% html_element("table")
```

## Exploramos el código html

Extraemos la tabla con html_table y guardamos como *data frame*

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
escuela<-escuela %>% html_table(fill=TRUE, header=TRUE, na.strings="")
escuela<-as.data.frame(escuela)
```

## Tenemos la información en bruto

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
escuela
```

## Extraemos los contenidos como vector

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
contenidos<-as.vector(escuela$Contenidos)
contenidos
```

## Separamos el título de la descripción como lista

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
lista_contenidos<-strsplit(contenidos, split = "\n")
lista_contenidos
```


## Se genera objeto clases con sapply

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
clases<-sapply(lista_contenidos,"[[",1)
clases
```

## Se genera objeto descripción con sapply

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
descripcion<-sapply(lista_contenidos,"[[",2)
descripcion
```

## Con herramientas tidyverse limpiamos datos

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
library('tidyverse')
descripcion<-str_squish(descripcion)
descripcion<-str_replace(descripcion, "\\s{2,}", "_")
descripcion
```

## Separamos las fechas

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
fechas<-as.vector(escuela$Fechas.de.clase)
lista_fechas<-strsplit(fechas, split = "\n")
lista_fechas
fechas_clases<-sapply(lista_fechas,"[[",1)
fechas_practica<-sapply(lista_fechas,"[[",2)
```


## Limpiamos y separamos las fechas

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
fechas_clases<-gsub(".*:","",fechas_clases)
fechas_clases<-str_squish(fechas_clases)
fechas_practica<-gsub(".*:","",fechas_practica)
fechas_practica<-str_squish(fechas_practica)
fechas_clases
fechas_practica
```

## Generamos dataframe y exportamos csv

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
escuela_datos<-as.data.frame(cbind(clases,fechas_clases,fechas_practica,descripcion))
write.table(escuela_datos,file="/home/ubuk/Escritorio/escueladatos.csv",sep=";", row.names=FALSE,quote=FALSE,col.names=TRUE)
```

---

::: {#fig-tablafinal}
![](img/tabla-final.png){width="80%"}

Tenemos nuestra tabla local
:::

## Ejemplo con IMDB: XML

-   XML (eXtensible Markup Language) es un lenguaje de marcado que sirve para estructurar y almacenar datos de forma legible tanto para humanos como para máquinas. Es similar a HTML, pero XML permite definir tus propias etiquetas, lo que lo hace adaptable a diferentes tipos de datos. Se usa para compartir información y puede ser usado para representar diversos tipos de datos, desde documentos y configuraciones hasta datos de bases de datos

## Requeriremos la librería XML

Existe un paquete XML y un paquete xml2. Este último es preferible al ser una versión más reciente.   

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
library('xml2')
```

## Obtenemos los elementos del sitio IMDB 

-   En nuestro navegador recurrimos a las herramientas para desarrolladores para identificar los elementos del sitio. 

-   En este caso podemos observar el elemento que contiene el título de la película *.ipc.title__text* o *.ipc-title-link-wrapper*  

---

::: {#fig-tablafinal}
![](img/imdb.png){width="80%"}

Sitio web IMDB 
:::

---

::: {#fig-tablafinal}
![](img/imdb2.png){width="78%"}

Sitio web IMDB con herramientas desarrolladores 
:::

## Encontramos limitaciones por uso de JS

-   En este caso, el sitio web contiene información html para las primeras 25 películas. El resto del contenido se agrega con JavScript. En este caso, por los alcances del curso, trabajaremos solo con los 25 elementos que es posible extraer directamente del componente html del código.

## Cargamos la base de datos

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
peliculas<-read_html("https://www.imdb.com/chart/top/")

peliculas %>% html_elements("table")
```

Con el inspector de elementos del navegador identificamos el elemento título con  *.ipc.title__text* o *.ipc-title-link-wrapper*.

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
titulo<-html_nodes(peliculas,".ipc-title-link-wrapper") %>% html_text
```

## Título de la película

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
titulo
```

## Luego extraemos los metadatos

Identificamos el nombre del nodo con el mismo método, inspeccionando en el navegador *.cli-title-metadata*. 

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
metadatos<-html_nodes(peliculas,".cli-title-metadata") %>% html_text
metadatos
```

## Extraemos los datos con substr

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
year<-as.numeric(substr(metadatos,1,4))
duracion<-substr(metadatos,5,9)
rating<-substr(metadatos,10,20)
top25<-as.data.frame(cbind(titulo,year,duracion,rating))
top25
```

## Ahora podemos analizar los datos

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1"
hist(year)
```